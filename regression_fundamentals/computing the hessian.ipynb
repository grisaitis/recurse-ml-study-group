{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T04:04:11.421350Z",
     "start_time": "2019-10-16T04:04:08.252160Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T04:04:11.474312Z",
     "start_time": "2019-10-16T04:04:11.469315Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/tensorflow/issues/29781#issuecomment-504980912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T04:05:22.179124Z",
     "start_time": "2019-10-16T04:05:22.158764Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([1.5110626 , 0.42292204], dtype=float32)>\n",
      "tf.Tensor([-0.75270414  0.30892995], shape=(2,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.65835893  0.27020815]\n",
      " [ 0.27020815  0.5754854 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def f(v):\n",
    "    return tf.sin(v[0] + tf.cos(v[1]))\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "x = tf.Variable(tf.random.normal((2,)))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as hess_tape:\n",
    "    with tf.GradientTape() as grad_tape:\n",
    "        y = f(x)\n",
    "    grad = grad_tape.gradient(y, x)\n",
    "    grad_grads = (hess_tape.gradient(g, x) for g in grad)\n",
    "    hess_columns = [gg[tf.newaxis, ...] for gg in grad_grads]\n",
    "    hessian = tf.concat(hess_columns, axis=0)\n",
    "\n",
    "print(x)\n",
    "print(grad)\n",
    "print(hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T03:32:13.271499Z",
     "start_time": "2019-10-16T03:32:13.253203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.65835893  0.27020815]\n",
      " [ 0.27020815  0.5754854 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def get_my_hessian(f, x):\n",
    "    with tf.GradientTape(persistent=True) as hess_tape:\n",
    "        with tf.GradientTape() as grad_tape:\n",
    "            y = f(x)\n",
    "        grad = grad_tape.gradient(y, x)\n",
    "        grad_grads = [hess_tape.gradient(g, x) for g in grad]\n",
    "    hess_rows = [gg[tf.newaxis, ...] for gg in grad_grads]\n",
    "    hessian = tf.concat(hess_rows, axis=0)\n",
    "    return hessian\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "x = tf.Variable(tf.random.normal((2,)))\n",
    "\n",
    "print(get_my_hessian(f, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T04:05:10.501089Z",
     "start_time": "2019-10-16T04:05:10.456833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=61, shape=(2,), dtype=float32, numpy=array([-0.38815078,  0.8456935 ], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = tf.Variable(tf.random.normal((2,)))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = tf.sin(v[0] + tf.cos(v[1]))\n",
    "    grads = tape.gradient(y, [v])\n",
    "\n",
    "hessians = tape.gradient(grads[0], [v])\n",
    "print(hessians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T03:10:58.781989Z",
     "start_time": "2019-10-16T03:10:58.761643Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.38815078  0.8456935 ], shape=(2,), dtype=float32)\n",
      "tf.Tensor([-0.65835893  0.27020815], shape=(2,), dtype=float32)\n",
      "tf.Tensor([0.27020815 0.5754854 ], shape=(2,), dtype=float32)\n",
      "tf.Tensor([-0.38815078  0.8456936 ], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = tf.Variable(tf.random.normal((2,)))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = tf.sin(v[0] + tf.cos(v[1]))\n",
    "    grad = tape.gradient(y, v)\n",
    "    hessian = tape.gradient(grad, v)\n",
    "    hessian0 = tape.gradient(grad[0], v)\n",
    "    hessian1 = tape.gradient(grad[1], v)\n",
    "\n",
    "print(hessian)\n",
    "print(hessian0)\n",
    "print(hessian1)\n",
    "print(hessian0 + hessian1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T03:18:57.156477Z",
     "start_time": "2019-10-16T03:18:56.715268Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-0.75270414  0.30892995], shape=(2,), dtype=float32)\n",
      "tf.Tensor([-0.38815078  0.8456935 ], shape=(2,), dtype=float32)\n",
      "tf.Tensor([-0.38815078  0.8456935 ], shape=(2,), dtype=float32)\n",
      "[<tf.Tensor: id=40578, shape=(), dtype=float32, numpy=-0.75270414>, <tf.Tensor: id=40579, shape=(), dtype=float32, numpy=0.30892995>]\n",
      "tf.Tensor([-0.38815078  0.8456935 ], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = tf.Variable(tf.random.normal((2,)))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape2:\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        y = tf.sin(v[0] + tf.cos(v[1]))\n",
    "    jacobian = tape.jacobian(y, v)\n",
    "    print(jacobian)\n",
    "    print(tape2.gradient(jacobian, v))\n",
    "    j_t = tf.transpose(jacobian)\n",
    "    print(tape2.gradient(j_t, v))\n",
    "    j_unstacked = tf.unstack(jacobian)\n",
    "    print(j_unstacked)\n",
    "    print(tape2.gradient(j_unstacked, v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T02:22:13.504074Z",
     "start_time": "2019-10-16T02:22:13.294245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "grad tf.Tensor([14. 42.], shape=(2,), dtype=float32)\n",
      "tf.Tensor([14. 42.], shape=(2,), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x[0] + 3 * x[1]) ** 2\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "v = tf.Variable(tf.constant([1, 2], dtype=tf.dtypes.float32))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = f(v)\n",
    "    grad = tape.gradient(y, v)\n",
    "    print(\"grad\", grad)\n",
    "#     print(tape.gradient(grad[0], v))\n",
    "#     print(tape.gradient(grad[1], v))\n",
    "\n",
    "jacobian = tape.jacobian(y, v)\n",
    "print(jacobian)\n",
    "x = tape.gradient(jacobian, v)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T02:32:56.801962Z",
     "start_time": "2019-10-16T02:32:56.778849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "grad\n",
      "tf.Tensor([-0.75270414  0.30892995], shape=(2,), dtype=float32)\n",
      "hessian\n",
      "tf.Tensor([-0.38815078  0.8456935 ], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = tf.Variable(tf.random.normal((2,)))\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = tf.sin(v[0] + tf.cos(v[1]))\n",
    "    grad = tape.gradient(y, v)\n",
    "\n",
    "hessian = tape.gradient(grad, v)\n",
    "\n",
    "print(\"grad\")\n",
    "print(grad)\n",
    "print(\"hessian\")\n",
    "print(hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T02:40:45.400368Z",
     "start_time": "2019-10-16T02:40:45.370743Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "hessian: tf.Tensor([-0.38815078  0.8456935 ], shape=(2,), dtype=float32)\n",
      "hessian0: tf.Tensor([-0.65835893  0.27020815], shape=(2,), dtype=float32)\n",
      "hessian1: tf.Tensor([0.27020815 0.5754854 ], shape=(2,), dtype=float32)\n",
      "hessian0 + hessian1: tf.Tensor([-0.38815078  0.8456936 ], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = tf.Variable(tf.random.normal((2,)))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = tf.sin(v[0] + tf.cos(v[1]))\n",
    "    grad = tape.gradient(y, v)\n",
    "    hessian = tape.gradient(grad, v)\n",
    "    hessian0 = tape.gradient(grad[0], v)\n",
    "    hessian1 = tape.gradient(grad[1], v)\n",
    "    \n",
    "print(\"hessian:\", hessian)\n",
    "print(\"hessian0:\", hessian0)\n",
    "print(\"hessian1:\", hessian1)\n",
    "print(\"hessian0 + hessian1:\", hessian0 + hessian1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T02:26:11.594958Z",
     "start_time": "2019-10-16T02:26:11.566085Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "grads [<tf.Tensor: id=15701, shape=(2,), dtype=float32, numpy=array([-0.75270414,  0.30892995], dtype=float32)>]\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "hess0 [<tf.Tensor: id=15730, shape=(2,), dtype=float32, numpy=array([-0.65835893,  0.27020815], dtype=float32)>]\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "hess1 [<tf.Tensor: id=15759, shape=(2,), dtype=float32, numpy=array([0.27020815, 0.5754854 ], dtype=float32)>]\n",
      "hessians [<tf.Tensor: id=15784, shape=(2,), dtype=float32, numpy=array([-0.38815078,  0.8456935 ], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "v = tf.Variable(tf.random.normal((2,)))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = tf.sin(v[0] + tf.cos(v[1]))\n",
    "    grads = tape.gradient(y, [v])\n",
    "    print(\"grads\", grads)\n",
    "    hess0 = tape.gradient(grads[0][0], [v])\n",
    "    print(\"hess0\", hess0)\n",
    "    hess1 = tape.gradient(grads[0][1], [v])\n",
    "    print(\"hess1\", hess1)\n",
    "\n",
    "hessians = tape.gradient(grads[0], [v])\n",
    "print(\"hessians\", hessians)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
